{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"c53980f3-43e6-4309-b5b3-8c387c94c2ef","_uuid":"ac363b82-57c3-4cb3-87ed-d2c3d463e1a5","trusted":true},"source":["# KAIM Week 8 and 9 Challenges"]},{"cell_type":"markdown","metadata":{"_cell_guid":"9597c964-28b3-47e3-9f51-0691092476ce","_uuid":"977dc833-b4c5-441a-9ca2-d7e8e86bedb1","trusted":true},"source":["## **Task 2: MOdel Building**"]},{"cell_type":"markdown","metadata":{"_cell_guid":"87a00a83-d488-4a01-a3c8-2d2d9f956d41","_uuid":"7ffdbba4-f7f1-4b89-92ea-16cc52919bc6","trusted":true},"source":["## Import Necessary Libraries"]},{"cell_type":"code","execution_count":44,"metadata":{"_cell_guid":"98500711-7c8e-4a73-a926-ad743b2ca99a","_uuid":"05984005-6423-4192-8e7f-856e08148d1f","collapsed":false,"execution":{"iopub.execute_input":"2024-10-18T11:28:10.886046Z","iopub.status.busy":"2024-10-18T11:28:10.885064Z","iopub.status.idle":"2024-10-18T11:28:12.528653Z","shell.execute_reply":"2024-10-18T11:28:12.527539Z","shell.execute_reply.started":"2024-10-18T11:28:10.886001Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Import necessary libraries\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","from sklearn.impute import SimpleImputer\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from xgboost import XGBClassifier\n","from sklearn.metrics import classification_report\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neural_network import MLPClassifier\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Conv2D, Flatten, SimpleRNN, LSTM\n","import mlflow\n","import mlflow.sklearn\n","import mlflow.xgboost\n","import warnings\n","\n","warnings.filterwarnings('ignore')\n","\n","# Set plot style for better visuals\n","sns.set(style=\"whitegrid\")"]},{"cell_type":"markdown","metadata":{"_cell_guid":"59a324ba-900b-40da-8f79-ee8f7957ef5b","_uuid":"8c78d907-d277-48a3-951f-ee7f01b6a7a7","trusted":true},"source":["## Load Datasets"]},{"cell_type":"code","execution_count":45,"metadata":{"_cell_guid":"98601976-817f-413e-ad3a-8c7eedb11606","_uuid":"b72ec9f6-c664-4339-8d29-555121dbd61f","collapsed":false,"execution":{"iopub.execute_input":"2024-10-18T11:28:15.198096Z","iopub.status.busy":"2024-10-18T11:28:15.197361Z","iopub.status.idle":"2024-10-18T11:28:20.098319Z","shell.execute_reply":"2024-10-18T11:28:20.097043Z","shell.execute_reply.started":"2024-10-18T11:28:15.198047Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["# Load the datasets\n","fraud_data = pd.read_csv('../data/cleaned_data_1.csv')\n","credit_data = pd.read_csv('../data/cleaned_data_2.csv')"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"data":{"text/plain":["((138846, 15), (283726, 31))"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["fraud_data.shape, credit_data.shape"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[{"data":{"text/plain":["(Index(['user_id', 'purchase_value', 'source', 'browser', 'sex', 'age',\n","        'ip_address', 'Class', 'country', 'lower_bound_ip_addres',\n","        'upper_bound_ip_adress', 'signup_purchase_diff', 'transaction_count',\n","        'hour_of_day', 'day_of_week'],\n","       dtype='object'),\n"," Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n","        'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n","        'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n","        'Class'],\n","       dtype='object'))"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["fraud_data.columns, credit_data.columns"]},{"cell_type":"markdown","metadata":{"_cell_guid":"a1c436e7-42e4-474c-8a8f-b00592a274ff","_uuid":"79aa1d6d-a15f-4224-815b-4344da833bbf","trusted":true},"source":["## Model Building"]},{"cell_type":"code","execution_count":48,"metadata":{"_cell_guid":"a49734b2-8a44-42d2-8b30-bd5a916c17eb","_uuid":"ab90e452-bf1c-4287-a72e-c1599293b5b7","collapsed":false,"execution":{"iopub.execute_input":"2024-10-18T12:12:29.677630Z","iopub.status.busy":"2024-10-18T12:12:29.677086Z","iopub.status.idle":"2024-10-18T12:13:18.305808Z","shell.execute_reply":"2024-10-18T12:13:18.304575Z","shell.execute_reply.started":"2024-10-18T12:12:29.677583Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(array([0, 1]), array([125849,  12997]))\n","(array([0, 1]), array([283253,    473]))\n"]}],"source":["# Prepare data for the model (e-commerce)\n","X1 = fraud_data.drop(columns=['Class'])\n","X2 = credit_data.drop(columns=['Class'])\n","\n","y1 = fraud_data['Class']\n","y2 = credit_data['Class']\n","\n","print(np.unique(y1, return_counts=True))\n","print(np.unique(y2, return_counts=True))"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[],"source":["X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size=0.2, random_state=42)\n","\n","X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.2, random_state=42)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Logistic Regression"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.91      1.00      0.95     25193\n","           1       0.00      0.00      0.00      2577\n","\n","    accuracy                           0.91     27770\n","   macro avg       0.45      0.50      0.48     27770\n","weighted avg       0.82      0.91      0.86     27770\n","\n"]}],"source":["log_reg = LogisticRegression(C=1, solver='liblinear')\n","log_reg.fit(X_train, y_train)\n","y_pred_log_reg = log_reg.predict(X_test)\n","print(classification_report(y_test, y_pred_log_reg))"]},{"cell_type":"markdown","metadata":{},"source":["### Random Forest"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.96      1.00      0.98     25193\n","           1       1.00      0.55      0.71      2577\n","\n","    accuracy                           0.96     27770\n","   macro avg       0.98      0.77      0.84     27770\n","weighted avg       0.96      0.96      0.95     27770\n","\n"]}],"source":["rf_clf = RandomForestClassifier(n_estimators=30, max_depth=3)\n","rf_clf.fit(X_train, y_train)\n","y_pred_rf = rf_clf.predict(X_test)\n","print(classification_report(y_test, y_pred_rf))"]},{"cell_type":"markdown","metadata":{},"source":["### XGBoost "]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.96      1.00      0.98     25193\n","           1       0.94      0.55      0.69      2577\n","\n","    accuracy                           0.96     27770\n","   macro avg       0.95      0.77      0.84     27770\n","weighted avg       0.95      0.96      0.95     27770\n","\n"]}],"source":["xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","xgb_clf.fit(X_train, y_train)\n","y_pred_xgb = xgb_clf.predict(X_test)\n","print(classification_report(y_test, y_pred_xgb))"]},{"cell_type":"markdown","metadata":{},"source":["#### Class Balancing using SMOTE"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(array([0, 1]), array([88947, 88947]))\n","(array([0, 1]), array([225994, 225994]))\n"]}],"source":["from imblearn.combine import SMOTETomek\n","\n","smt = SMOTETomek(random_state=42)\n","X_train_res1, y_train_res1 = smt.fit_resample(X_train1, y_train1)\n","X_train_res2, y_train_res2 = smt.fit_resample(X_train2, y_train2)\n","\n","print(np.unique(y_train_res1, return_counts=True))\n","print(np.unique(y_train_res2, return_counts=True))"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.96      0.99      0.97     25193\n","           1       0.81      0.56      0.66      2577\n","\n","    accuracy                           0.95     27770\n","   macro avg       0.88      0.77      0.82     27770\n","weighted avg       0.94      0.95      0.94     27770\n","\n"]}],"source":["xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","xgb_clf.fit(X_train_res, y_train_res)\n","y_pred_xgb = xgb_clf.predict(X_test)\n","print(classification_report(y_test, y_pred_xgb))"]},{"cell_type":"markdown","metadata":{},"source":["## Experiments"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[],"source":["datasets = {\n","    \"fraud_data\": (X_train1, y_train1, X_test1, y_test1),\n","    \"credit_data\": (X_train2, y_train2, X_test2, y_test2),\n","}"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[{"data":{"text/plain":["'\\nmodels = [\\n    (\\n        \"Logistic Regression\", \\n        LogisticRegression(C=1, solver=\\'liblinear\\'), \\n        (X_train, y_train),\\n        (X_test, y_test)\\n    ),\\n    (\\n        \"Random Forest\", \\n        RandomForestClassifier(n_estimators=30, max_depth=3), \\n        (X_train, y_train),\\n        (X_test, y_test)\\n    ),\\n    (\\n        \"XGBClassifier\",\\n        XGBClassifier(use_label_encoder=False, eval_metric=\\'logloss\\'), \\n        (X_train, y_train),\\n        (X_test, y_test)\\n    ),\\n    (\\n        \"XGBClassifier With SMOTE\",\\n        XGBClassifier(use_label_encoder=False, eval_metric=\\'logloss\\'), \\n        (X_train_res1, y_train_res1),\\n        (X_test1, y_test1)\\n    )\\n]\\n'"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","models = [\n","    (\n","        \"Logistic Regression\", \n","        LogisticRegression(C=1, solver='liblinear'), \n","        (X_train, y_train),\n","        (X_test, y_test)\n","    ),\n","    (\n","        \"Random Forest\", \n","        RandomForestClassifier(n_estimators=30, max_depth=3), \n","        (X_train, y_train),\n","        (X_test, y_test)\n","    ),\n","    (\n","        \"XGBClassifier\",\n","        XGBClassifier(use_label_encoder=False, eval_metric='logloss'), \n","        (X_train, y_train),\n","        (X_test, y_test)\n","    ),\n","    (\n","        \"XGBClassifier With SMOTE\",\n","        XGBClassifier(use_label_encoder=False, eval_metric='logloss'), \n","        (X_train_res1, y_train_res1),\n","        (X_test1, y_test1)\n","    )\n","]\n","'''"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"object __array__ method not producing an array","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[53], line 56\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Defining each model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m models \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Logistic Regression\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     (\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogistic Regression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m      6\u001b[0m         LogisticRegression(C\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, solver\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m     ),\n\u001b[0;32m      8\u001b[0m     \n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Random Forest\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     (\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandom Forest\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     12\u001b[0m         RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     13\u001b[0m     ),\n\u001b[0;32m     14\u001b[0m     \n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# XGBoost\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     (\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXGBClassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     18\u001b[0m         XGBClassifier(use_label_encoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogloss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     19\u001b[0m     ),\n\u001b[0;32m     20\u001b[0m \n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# Decision Tree\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     (\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecision Tree\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     24\u001b[0m         DecisionTreeClassifier(max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     25\u001b[0m     ),\n\u001b[0;32m     26\u001b[0m     \n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# Multi-Layer Perceptron (MLP)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     (\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMLP Classifier\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     30\u001b[0m         MLPClassifier(hidden_layer_sizes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m100\u001b[39m,), max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m)\n\u001b[0;32m     31\u001b[0m     ),\n\u001b[0;32m     32\u001b[0m     \n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# Convolutional Neural Network (CNN)\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     (\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCNN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     36\u001b[0m         Sequential([\n\u001b[0;32m     37\u001b[0m             Conv2D(\u001b[38;5;241m32\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m1\u001b[39m)),\n\u001b[0;32m     38\u001b[0m             Flatten(),\n\u001b[0;32m     39\u001b[0m             Dense(\u001b[38;5;241m128\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     40\u001b[0m             Dense(\u001b[38;5;241m1\u001b[39m , activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     41\u001b[0m         ])\n\u001b[0;32m     42\u001b[0m     ),\n\u001b[0;32m     43\u001b[0m     \n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# Recurrent Neural Network (RNN)\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     (\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRNN\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     47\u001b[0m         Sequential([\n\u001b[0;32m     48\u001b[0m             SimpleRNN(\u001b[38;5;241m50\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m1\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     49\u001b[0m             Dense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     50\u001b[0m         ])\n\u001b[0;32m     51\u001b[0m     ),\n\u001b[0;32m     52\u001b[0m     \n\u001b[0;32m     53\u001b[0m     \u001b[38;5;66;03m# Long Short-Term Memory (LSTM)\u001b[39;00m\n\u001b[0;32m     54\u001b[0m     (\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m---> 56\u001b[0m         \u001b[43mSequential\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m            \u001b[49m\u001b[43mLSTM\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m            \u001b[49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msigmoid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m     )\n\u001b[0;32m     61\u001b[0m ]\n","File \u001b[1;32mc:\\Users\\hp\\KAIM\\KAIM-W8-9\\.week89\\lib\\site-packages\\keras\\src\\models\\sequential.py:76\u001b[0m, in \u001b[0;36mSequential.__init__\u001b[1;34m(self, layers, trainable, name)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m layers:\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd(layer, rebuild\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_rebuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\hp\\KAIM\\KAIM-W8-9\\.week89\\lib\\site-packages\\keras\\src\\models\\sequential.py:141\u001b[0m, in \u001b[0;36mSequential._maybe_rebuild\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m0\u001b[39m], InputLayer) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    140\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbatch_shape\n\u001b[1;32m--> 141\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;66;03m# We can build the Sequential model if the first layer has the\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;66;03m# `input_shape` property. This is most commonly found in Functional\u001b[39;00m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;66;03m# model.\u001b[39;00m\n\u001b[0;32m    146\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39minput_shape\n","File \u001b[1;32mc:\\Users\\hp\\KAIM\\KAIM-W8-9\\.week89\\lib\\site-packages\\keras\\src\\layers\\layer.py:226\u001b[0m, in \u001b[0;36mLayer.__new__.<locals>.build_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_open_name_scope():\n\u001b[0;32m    225\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_path \u001b[38;5;241m=\u001b[39m current_path()\n\u001b[1;32m--> 226\u001b[0m     original_build_method(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    227\u001b[0m \u001b[38;5;66;03m# Record build config.\u001b[39;00m\n\u001b[0;32m    228\u001b[0m signature \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(original_build_method)\n","File \u001b[1;32mc:\\Users\\hp\\KAIM\\KAIM-W8-9\\.week89\\lib\\site-packages\\keras\\src\\models\\sequential.py:187\u001b[0m, in \u001b[0;36mSequential.build\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 187\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[0;32m    189\u001b[0m         \u001b[38;5;66;03m# Can happen if shape inference is not implemented.\u001b[39;00m\n\u001b[0;32m    190\u001b[0m         \u001b[38;5;66;03m# TODO: consider reverting inbound nodes on layers processed.\u001b[39;00m\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\hp\\KAIM\\KAIM-W8-9\\.week89\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[1;32mc:\\Users\\hp\\KAIM\\KAIM-W8-9\\.week89\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\numpy.py:2414\u001b[0m, in \u001b[0;36mtranspose\u001b[1;34m(x, axes)\u001b[0m\n\u001b[0;32m   2412\u001b[0m     output\u001b[38;5;241m.\u001b[39mset_shape(compute_transpose_output_shape(x\u001b[38;5;241m.\u001b[39mshape, axes))\n\u001b[0;32m   2413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m-> 2414\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxes\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[1;31mValueError\u001b[0m: object __array__ method not producing an array"]}],"source":["# Defining each model\n","models = [\n","    # Logistic Regression\n","    (\n","        \"Logistic Regression\", \n","        LogisticRegression(C=1, solver='liblinear')\n","    ),\n","    \n","    # Random Forest\n","    (\n","        \"Random Forest\", \n","        RandomForestClassifier(n_estimators=30, max_depth=3)\n","    ),\n","    \n","    # XGBoost\n","    (\n","        \"XGBClassifier\", \n","        XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","    ),\n","\n","    # Decision Tree\n","    (\n","        \"Decision Tree\", \n","        DecisionTreeClassifier(max_depth=5)\n","    ),\n","    \n","    # Multi-Layer Perceptron (MLP)\n","    (\n","        \"MLP Classifier\", \n","        MLPClassifier(hidden_layer_sizes=(100,), max_iter=300)\n","    ),\n","    \n","    # Convolutional Neural Network (CNN)\n","    (\n","        \"CNN\", \n","        Sequential([\n","            Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n","            Flatten(),\n","            Dense(128, activation='relu'),\n","            Dense(1 , activation='sigmoid')\n","        ])\n","    ),\n","    \n","    # Recurrent Neural Network (RNN)\n","    (\n","        \"RNN\", \n","        Sequential([\n","            SimpleRNN(50, input_shape=(100, 1), activation='relu'),\n","            Dense(1, activation='sigmoid')\n","        ])\n","    ),\n","    \n","    # Long Short-Term Memory (LSTM)\n","    (\n","        \"LSTM\", \n","        Sequential([\n","            LSTM(50, input_shape=(100, 1), activation='relu'),\n","            Dense(1, activation='sigmoid')\n","        ])\n","    )\n","]\n"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[],"source":["models = [\n","    (\n","        \"Logistic Regression\", \n","        LogisticRegression(C=1, solver='liblinear')\n","    ),\n","    (\n","        \"Random Forest\", \n","        RandomForestClassifier(n_estimators=30, max_depth=3)\n","    ),\n","    (\n","        \"XGBClassifier\",\n","        XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","    )\n","]"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2024/10/20 13:43:16 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n","2024/10/20 13:43:16 INFO mlflow.tracking._tracking_service.client: 🏃 View run Logistic Regression at: http://localhost:5000/#/experiments/935012191474515353/runs/66416527b5ad44a0be0886ed9e43b658.\n","2024/10/20 13:43:16 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/935012191474515353.\n","2024/10/20 13:43:20 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n","2024/10/20 13:43:21 INFO mlflow.tracking._tracking_service.client: 🏃 View run Random Forest at: http://localhost:5000/#/experiments/935012191474515353/runs/3c3923af256d4ca4947318c3e70a6fbf.\n","2024/10/20 13:43:21 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/935012191474515353.\n","2024/10/20 13:43:25 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n","2024/10/20 13:43:25 INFO mlflow.tracking._tracking_service.client: 🏃 View run XGBClassifier at: http://localhost:5000/#/experiments/935012191474515353/runs/019f76c5419a4e24a15a654ad2358b1c.\n","2024/10/20 13:43:25 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/935012191474515353.\n","2024/10/20 13:43:28 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n","2024/10/20 13:43:29 INFO mlflow.tracking._tracking_service.client: 🏃 View run XGBClassifier With SMOTE at: http://localhost:5000/#/experiments/935012191474515353/runs/b89994c7190a441495941622db7bf9ac.\n","2024/10/20 13:43:29 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/935012191474515353.\n"]}],"source":["# Initialize MLflow\n","mlflow.set_experiment(\"Fraud Detection Models - Single Dataset\")\n","mlflow.set_tracking_uri(\"http://localhost:5000\")\n","\n","reports = []\n","\n","for model_name, model, train_set, test_set in models:\n","    X_train = train_set[0]\n","    y_train = train_set[1]\n","    X_test = test_set[0]\n","    y_test = test_set[1]\n","    \n","    model.fit(X_train, y_train)\n","    y_pred = model.predict(X_test)\n","    report = classification_report(y_test, y_pred, output_dict=True)\n","    reports.append(report)\n","\n","for i, element in enumerate(models):\n","    model_name = element[0]\n","    model = element[1]\n","    report = reports[i]\n","    \n","    with mlflow.start_run(run_name=model_name):        \n","        mlflow.log_param(\"model\", model_name)\n","        mlflow.log_metric('accuracy', report['accuracy'])\n","        mlflow.log_metric('recall_class_1', report['1']['recall'])\n","        mlflow.log_metric('recall_class_0', report['0']['recall'])\n","        mlflow.log_metric('f1_score_macro', report['macro avg']['f1-score'])        \n","        \n","        if \"XGB\" in model_name:\n","            mlflow.xgboost.log_model(model, \"model\")\n","        else:\n","            mlflow.sklearn.log_model(model, \"model\")  "]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2024/10/20 20:16:21 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n","2024/10/20 20:16:21 INFO mlflow.tracking._tracking_service.client: 🏃 View run Logistic Regression_fraud_data at: http://localhost:5000/#/experiments/478268722598582565/runs/8273f777699647a39535d873391b9d13.\n","2024/10/20 20:16:21 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/478268722598582565.\n","2024/10/20 20:16:24 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n","2024/10/20 20:16:24 INFO mlflow.tracking._tracking_service.client: 🏃 View run Random Forest_fraud_data at: http://localhost:5000/#/experiments/478268722598582565/runs/65913aaab7ce49948b90f6752b771312.\n","2024/10/20 20:16:24 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/478268722598582565.\n","2024/10/20 20:16:27 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n","2024/10/20 20:16:28 INFO mlflow.tracking._tracking_service.client: 🏃 View run XGBClassifier_fraud_data at: http://localhost:5000/#/experiments/478268722598582565/runs/e262b2c32c8442b2a720a086f36a2f63.\n","2024/10/20 20:16:28 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/478268722598582565.\n","2024/10/20 20:16:52 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n","2024/10/20 20:16:52 INFO mlflow.tracking._tracking_service.client: 🏃 View run Logistic Regression_credit_data at: http://localhost:5000/#/experiments/478268722598582565/runs/37abcc1c28b54206bffc2f7695ca9d49.\n","2024/10/20 20:16:52 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/478268722598582565.\n","2024/10/20 20:16:56 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n","2024/10/20 20:16:56 INFO mlflow.tracking._tracking_service.client: 🏃 View run Random Forest_credit_data at: http://localhost:5000/#/experiments/478268722598582565/runs/afef239a5b3845b082ab9368d9621edd.\n","2024/10/20 20:16:56 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/478268722598582565.\n","2024/10/20 20:16:59 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n","2024/10/20 20:16:59 INFO mlflow.tracking._tracking_service.client: 🏃 View run XGBClassifier_credit_data at: http://localhost:5000/#/experiments/478268722598582565/runs/bcfb79f52acc4f82bf61298763c208df.\n","2024/10/20 20:16:59 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://localhost:5000/#/experiments/478268722598582565.\n"]}],"source":["# Initialize MLflow\n","mlflow.set_experiment(\"Fraud Detection Models - 2 Datasets\")\n","mlflow.set_tracking_uri(\"http://localhost:5000\")\n","\n","# Iterate through datasets\n","for dataset_name, (X_train, y_train, X_test, y_test) in datasets.items():\n","    reports = []\n","\n","    # Train each model on the current dataset\n","    for model_name, model in models:\n","        model.fit(X_train, y_train)\n","        y_pred = model.predict(X_test)\n","        report = classification_report(y_test, y_pred, output_dict=True)\n","        reports.append(report)\n","\n","    # Log each model's performance metrics to MLflow\n","    for i, (model_name, model) in enumerate(models):\n","        report = reports[i]\n","\n","        with mlflow.start_run(run_name=f\"{model_name}_{dataset_name}\"):\n","            mlflow.log_param(\"model\", model_name)\n","            mlflow.log_param(\"dataset\", dataset_name)\n","            mlflow.log_metric('accuracy', report['accuracy'])\n","            mlflow.log_metric('recall_class_1', report['1']['recall'])\n","            mlflow.log_metric('recall_class_0', report['0']['recall'])\n","            mlflow.log_metric('f1_score_macro', report['macro avg']['f1-score'])\n","\n","            # Log the model using the appropriate MLflow method\n","            if \"XGBoost\" in model_name:\n","                mlflow.xgboost.log_model(model, \"model\")\n","            else:\n","                mlflow.sklearn.log_model(model, \"model\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Load the datasets\n","fraud_data = pd.read_csv('../data/cleaned_data_1.csv')\n","credit_data = pd.read_csv('../data/cleaned_data_2.csv')\n","\n","# Prepare data for the model (e-commerce)\n","X1 = fraud_data.drop(columns=['Class'])\n","X2 = credit_data.drop(columns=['Class'])\n","\n","y1 = fraud_data['Class']\n","y2 = credit_data['Class']\n","\n","print(np.unique(y1, return_counts=True))\n","print(np.unique(y2, return_counts=True))\n","\n","# Train-test split\n","X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size=0.2, random_state=42)\n","X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.2, random_state=42)\n","\n","\n","# Datasets\n","datasets = {\n","    \"fraud_data\": (X_train1, y_train1, X_test1, y_test1),\n","    \"credit_data\": (X_train2, y_train2, X_test2, y_test2),\n","}\n","\n","\n","# Defining each model\n","models = [\n","    # Logistic Regression\n","    (\n","        \"Logistic Regression\", \n","        LogisticRegression(C=1, solver='liblinear')\n","    ),\n","    \n","    # Random Forest\n","    (\n","        \"Random Forest\", \n","        RandomForestClassifier(n_estimators=30, max_depth=3)\n","    ),\n","    \n","    # XGBoost\n","    (\n","        \"XGBClassifier\", \n","        XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","    ),\n","\n","    # Decision Tree\n","    (\n","        \"Decision Tree\", \n","        DecisionTreeClassifier(max_depth=5)\n","    ),\n","    \n","    # Multi-Layer Perceptron (MLP)\n","    (\n","        \"MLP Classifier\", \n","        MLPClassifier(hidden_layer_sizes=(100,), max_iter=300)\n","    ),\n","    \n","    # Convolutional Neural Network (CNN)\n","    (\n","        \"CNN\", \n","        Sequential([\n","            Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n","            Flatten(),\n","            Dense(128, activation='relu'),\n","            Dense(1 , activation='sigmoid')\n","        ])\n","    ),\n","    \n","    # Recurrent Neural Network (RNN)\n","    (\n","        \"RNN\", \n","        Sequential([\n","            SimpleRNN(50, input_shape=(100, 1), activation='relu'),\n","            Dense(1, activation='sigmoid')\n","        ])\n","    ),\n","    \n","    # Long Short-Term Memory (LSTM)\n","    (\n","        \"LSTM\", \n","        Sequential([\n","            LSTM(50, input_shape=(100, 1), activation='relu'),\n","            Dense(1, activation='sigmoid')\n","        ])\n","    )\n","]\n","\n","\n","\n","# Initialize MLflow\n","mlflow.set_experiment(\"Fraud Detection Models - 2 Datasets\")\n","mlflow.set_tracking_uri(\"http://localhost:5000\")\n","\n","# Iterate through datasets\n","for dataset_name, (X_train, y_train, X_test, y_test) in datasets.items():\n","    reports = []\n","\n","    # Train each model on the current dataset\n","    for model_name, model in models:\n","        model.fit(X_train, y_train)\n","        y_pred = model.predict(X_test)\n","        report = classification_report(y_test, y_pred, output_dict=True)\n","        reports.append(report)\n","\n","    # Log each model's performance metrics to MLflow\n","    for i, (model_name, model) in enumerate(models):\n","        report = reports[i]\n","\n","        with mlflow.start_run(run_name=f\"{model_name}_{dataset_name}\"):\n","            mlflow.log_param(\"model\", model_name)\n","            mlflow.log_param(\"dataset\", dataset_name)\n","            mlflow.log_metric('accuracy', report['accuracy'])\n","            mlflow.log_metric('recall_class_1', report['1']['recall'])\n","            mlflow.log_metric('recall_class_0', report['0']['recall'])\n","            mlflow.log_metric('f1_score_macro', report['macro avg']['f1-score'])\n","\n","            # Log the model using the appropriate MLflow method\n","            if \"XGBoost\" in model_name:\n","                mlflow.xgboost.log_model(model, \"model\")\n","            else:\n","                mlflow.sklearn.log_model(model, \"model\")\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5896976,"sourceId":9653739,"sourceType":"datasetVersion"}],"dockerImageVersionId":30786,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":".week89","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"}},"nbformat":4,"nbformat_minor":4}
